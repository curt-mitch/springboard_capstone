{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder model for Japanese-to-English Translation\n",
    "inspired by: https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import mojimoji\n",
    "from spacy.lang.ja import Japanese\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[w for w in l.split('\\t')]  for l in lines]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, jp = create_dataset('./jesc-corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Japanese text (since Japanese doesn't naturally put spaces between words)\n",
    "\n",
    "# reference: https://github.com/WorksApplications/SudachiPy\n",
    "# Load SudachiPy with split mode B: \"国家公務員\" => ['国家', '公務員']\n",
    "# default is split mode A: \"国家公務員\" => ['国家公務員']\n",
    "# NOTE: this may be worth adjusting in future training\n",
    "jcfg = {\"split_mode\": \"B\"}\n",
    "j_tokenizer = Japanese(meta={\"tokenizer\": {\"config\": jcfg}})\n",
    "\n",
    "def tokenize_jp_sentence(text):\n",
    "    return \" \".join([i.text for i in j_tokenizer(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あなたは戻ったのね ハロルド?\n",
      "あなた は 戻っ た の ね ハロルド ?\n"
     ]
    }
   ],
   "source": [
    "print(jp[0])\n",
    "print(tokenize_jp_sentence(jp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert any half-width katakana to normal-width katakana using mojimoji library\n",
    "def norm_kt(text):\n",
    "    return mojimoji.han_to_zen(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ﾆｭｰﾗﾙﾈｯﾄﾜｰｸ: ニューラルネットワーク\n"
     ]
    }
   ],
   "source": [
    "print(\"ﾆｭｰﾗﾙﾈｯﾄﾜｰｸ: \" + norm_kt(\"ﾆｭｰﾗﾙﾈｯﾄﾜｰｸ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode to ascii\n",
    "def jp_unicode_to_ascii(text):\n",
    "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))\n",
    "\n",
    "# remove any accented characters for English-language text\n",
    "def en_unicode_to_ascii(text):\n",
    "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text)\n",
    "                   .encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's in my resume.\n",
      "それは履歴書にあります。\n"
     ]
    }
   ],
   "source": [
    "print(en_unicode_to_ascii(\"It's in my résumé.\"))\n",
    "print(jp_unicode_to_ascii(\"それは履歴書にあります。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only Kanji, Hiragana, Katakana, numerals, and common punctuation: (\"。\", \"、\", \"?\", \"？\", \"!\", \"！\"))\n",
    "def jp_preprocessing_and_spacing(text):\n",
    "    text = re.sub(r\"([。、?？!！])\", r\" \\1\", text)\n",
    "    pattern = r\"[^\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!！\\s、。.,0-9]+\"\n",
    "    text = re.sub(pattern, '', text).rstrip().strip()\n",
    "\n",
    "    # add spaces between words and punctuation\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    # remove interpunct (黒丸)\n",
    "    text = text.replace(\"・\" , \"\")\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove special characters and place spaces between words and punctuation\n",
    "def en_preprocessing_and_spacing(text):\n",
    "    text = en_unicode_to_ascii(text.lower().strip())\n",
    "\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello , email world !\n",
      "こんにちは 、エメール世界 ！\n"
     ]
    }
   ],
   "source": [
    "print(en_preprocessing_and_spacing('Hello, email@world!'))\n",
    "print(jp_preprocessing_and_spacing('こんにちは、エメール＠世界！'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize preprocessing functions and mark start and end of sentences\n",
    "def normalize_text(japanese_text, english_text):\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for jp_text, en_text in tqdm(zip(japanese_text, english_text)):\n",
    "\n",
    "        # normalize Japanese\n",
    "        jp_text = jp_unicode_to_ascii(jp_text)\n",
    "        jp_text = jp_preprocessing_and_spacing(jp_text)\n",
    "        jp_text = tokenize_jp_sentence(jp_text)\n",
    "        jp_text = norm_kt(jp_text)\n",
    "\n",
    "        jp_text = \"<start> \" + jp_text + \" <end>\"\n",
    "        \n",
    "        inputs.append(jp_text)\n",
    "        \n",
    "        # normalize English\n",
    "        en_text = en_unicode_to_ascii(en_text)\n",
    "        en_text = en_preprocessing_and_spacing(en_text)\n",
    "\n",
    "        en_text = \"<start> \" + en_text + \" <end>\"\n",
    "        targets.append(en_text)\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2801388it [56:02, 833.23it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = normalize_text(jp, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> あなた　は　戻っ　た　の　ね　ハロルド　？ <end>\n",
      "<start> you are back , aren t you , harold ? <end>\n",
      "<start> 俺　の　相手　は　シャーク　だ　。 <end>\n",
      "<start> my opponent is shark . <end>\n",
      "<start> 引き換え　だ　ある　事　と　ある　物　の <end>\n",
      "<start> this is one thing in exchange for another . <end>\n",
      "<start> もう　いい　よ　ごちそう　さま　ううん <end>\n",
      "<start> yeah , i m fine . <end>\n",
      "<start> もう　会社　に　は　来　ない　で　くれ　電話　も　する　な <end>\n",
      "<start> don t come to the office anymore . don t call me either . <end>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(inputs[i])\n",
    "    print(targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jp_normalized.txt', 'w') as filehandle:\n",
    "    for jp_sentence in jp:\n",
    "        filehandle.write('%s\\n' % jp_sentence)\n",
    "\n",
    "with open('en_normalized.txt', 'w') as filehandle:\n",
    "    for en_sentence in jp:\n",
    "        filehandle.write('%s\\n' % en_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "\n",
    "    # updates internal vocabulary based on a corpus\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    # Pads sequences to the same length.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カンパニーの元社員が\n",
      "it seems a former employee...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2, 4, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 5, 6, 7, 8, 9, 1, 1, 1, 3]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x7f931dbaaf90>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(jp[9])\n",
    "print(en[9])\n",
    "tokenize([inputs[9], targets[9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, input_lang_tokenizer = tokenize(inputs)\n",
    "target_tensor, target_lang_tokenizer = tokenize(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate max_length of the target tensors\n",
    "max_length_target, max_length_input = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2101041 2101041 420208 420208 280139 280139\n"
     ]
    }
   ],
   "source": [
    "# Creating train-test-validation splits\n",
    "# Reference: https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is 75% of the entire data set\n",
    "input_tensor_train, input_tensor_test, \\\n",
    "    target_tensor_train, target_tensor_test = train_test_split(input_tensor, target_tensor,\n",
    "                                                               test_size=1 - train_ratio,\n",
    "                                                               random_state=1)\n",
    "\n",
    "# test is 10% of the initial data set\n",
    "# validation is 15% of the initial data set\n",
    "input_tensor_val, input_tensor_test, \\\n",
    "    target_tensor_val, target_tensor_test = train_test_split(input_tensor_test, target_tensor_test,\n",
    "                                                             test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), \n",
    "      len(input_tensor_val), len(target_tensor_val), \n",
    "      len(input_tensor_test), len(target_tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "500159 ----> おはよう　ジョシュ　やあ　ローズ\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "86 ----> good\n",
      "305 ----> morning\n",
      "4 ----> ,\n",
      "4941 ----> josh\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(input_lang_tokenizer, input_tensor_train[1])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(target_lang_tokenizer, target_tensor_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 3]), TensorShape([64, 65]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Encoder and Decoder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 3, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference for Bahdanau Attention Encoder: https://arxiv.org/pdf/1409.0473.pdf\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 16339)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.4529\n",
      "Epoch 1 Batch 100 Loss 0.8561\n",
      "Epoch 1 Batch 200 Loss 0.7882\n",
      "Epoch 1 Batch 300 Loss 0.7166\n",
      "Epoch 1 Loss 0.8160\n",
      "Time taken for 1 epoch 2466.6977412700653 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7311\n",
      "Epoch 2 Batch 100 Loss 0.6527\n",
      "Epoch 2 Batch 200 Loss 0.6626\n",
      "Epoch 2 Batch 300 Loss 0.7157\n",
      "Epoch 2 Loss 0.7039\n",
      "Time taken for 1 epoch 2360.282642841339 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7072\n",
      "Epoch 3 Batch 100 Loss 0.6414\n",
      "Epoch 3 Batch 200 Loss 0.7423\n",
      "Epoch 3 Batch 300 Loss 0.5559\n",
      "Epoch 3 Loss 0.6590\n",
      "Time taken for 1 epoch 2358.7948939800262 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6156\n",
      "Epoch 4 Batch 100 Loss 0.6339\n",
      "Epoch 4 Batch 200 Loss 0.6402\n",
      "Epoch 4 Batch 300 Loss 0.6375\n",
      "Epoch 4 Loss 0.6201\n",
      "Time taken for 1 epoch 2415.572633266449 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6749\n",
      "Epoch 5 Batch 100 Loss 0.5432\n",
      "Epoch 5 Batch 200 Loss 0.5254\n",
      "Epoch 5 Batch 300 Loss 0.6183\n",
      "Epoch 5 Loss 0.5738\n",
      "Time taken for 1 epoch 2506.854194879532 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5429\n",
      "Epoch 6 Batch 100 Loss 0.4749\n",
      "Epoch 6 Batch 200 Loss 0.5093\n",
      "Epoch 6 Batch 300 Loss 0.5599\n",
      "Epoch 6 Loss 0.5189\n",
      "Time taken for 1 epoch 2433.934334039688 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4735\n",
      "Epoch 7 Batch 100 Loss 0.3856\n",
      "Epoch 7 Batch 200 Loss 0.4752\n",
      "Epoch 7 Batch 300 Loss 0.4520\n",
      "Epoch 7 Loss 0.4546\n",
      "Time taken for 1 epoch 2461.7706639766693 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.3507\n",
      "Epoch 8 Batch 100 Loss 0.3173\n",
      "Epoch 8 Batch 200 Loss 0.3412\n",
      "Epoch 8 Batch 300 Loss 0.3914\n",
      "Epoch 8 Loss 0.3832\n",
      "Time taken for 1 epoch 2429.861626148224 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3073\n",
      "Epoch 9 Batch 100 Loss 0.2354\n",
      "Epoch 9 Batch 200 Loss 0.2705\n",
      "Epoch 9 Batch 300 Loss 0.3611\n",
      "Epoch 9 Loss 0.3138\n",
      "Time taken for 1 epoch 2424.4825427532196 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2549\n",
      "Epoch 10 Batch 100 Loss 0.2207\n",
      "Epoch 10 Batch 200 Loss 0.2124\n",
      "Epoch 10 Batch 300 Loss 0.2565\n",
      "Epoch 10 Loss 0.2496\n",
      "Time taken for 1 epoch 2433.772400856018 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                        batch,\n",
    "                                                        batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa9f76ef050>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move checkpoints into base model\n",
    "encoder = checkpoint.encoder\n",
    "decoder = checkpoint.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = tf.convert_to_tensor(sentence)\n",
    "    inputs = tf.expand_dims(inputs, axis=0)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, state = encoder(inputs, hidden)\n",
    "    hidden_state = state\n",
    "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, hidden_state, _ = decoder(dec_input,\n",
    "                                            hidden_state,\n",
    "                                            enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenizer.index_word[predicted_id] == '<end>' or len(result) > max_length_target:\n",
    "            return result\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference(lang, tensor):\n",
    "    all_sentence_list = []\n",
    "\n",
    "    for word_list in tensor:\n",
    "        sentence_list = []\n",
    "\n",
    "        for t in word_list:\n",
    "            if not t == 0:\n",
    "                # Index number assigned to each word\n",
    "                sentence_list.append(lang.index_word[t])\n",
    "        all_sentence_list.append(sentence_list)\n",
    "    return all_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = create_reference(target_lang_tokenizer, target_tensor_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [09:12<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# create predictions\n",
    "predictions = []\n",
    "for test in tqdm(input_tensor_test):\n",
    "    predictions.append(predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average meteor score per sentence is: 0.06839\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "for i in range(len(reference)):\n",
    "    score += single_meteor_score(\" \".join(reference[i][1:-1]), predictions[i][:-5])\n",
    "\n",
    "score /= len(reference)\n",
    "print(\"The average meteor score per sentence is: {:1.5f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start>', 1),\n",
       " ('<end>', 2),\n",
       " ('ありがとう', 3),\n",
       " ('！', 4),\n",
       " ('了解', 5),\n",
       " ('ああ', 6),\n",
       " ('．', 7),\n",
       " ('．\\u3000．\\u3000．', 8),\n",
       " ('どうぞ', 9),\n",
       " ('その\\u3000通り', 10)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "take(10, input_lang_tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: いい天気ですね\n",
      "2: いい天気ですね\n",
      "3: いい 天気 です ね\n",
      "4: いい　天気　です　ね\n",
      "5: <start>　いい　天気　です　ね　<end>\n",
      "6: ['<start>', 'いい', '天気', 'です', 'ね', '<end>']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'いい'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-f44f9b2cc425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'6: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\u3000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# [input_lang_tokenizer.word_index[i] for i in sentence.split('\\u3000')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minput_lang_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'いい'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'いい'"
     ]
    }
   ],
   "source": [
    "# debugging normalize Japanese for evaluate method\n",
    "# sentence = jp_unicode_to_ascii(\"いい天気ですね\")\n",
    "# sentence = jp_preprocessing_and_spacing(sentence)\n",
    "# sentence = tokenize_jp_sentence(sentence)\n",
    "# sentence = norm_kt(sentence)\n",
    "\n",
    "# sentence = \"<start>\\u3000\" + sentence + \"\\u3000<end>\"\n",
    "# # [input_lang_tokenizer.word_index[i] for i in sentence.split('\\u3000')]\n",
    "# # input_lang_tokenizer.word_index['いい']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    # normalize Japanese\n",
    "    sentence = jp_unicode_to_ascii(sentence)\n",
    "    sentence = jp_preprocessing_and_spacing(sentence)\n",
    "    sentence = tokenize_jp_sentence(sentence)\n",
    "    sentence = norm_kt(sentence)\n",
    "\n",
    "    sentence = \"<start>\\u3000\" + sentence + \"\\u3000<end>\"\n",
    "    print(sentence)\n",
    "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split('\\u3000')]\n",
    "\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_input,\n",
    "                                                           padding='post')\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, state = encoder(inputs, hidden)\n",
    "    hidden_state = state\n",
    "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, hidden_state, _ = decoder(dec_input,\n",
    "                                                hidden_state,\n",
    "                                                enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenizer.index_word[predicted_id] == '<end>' or len(result) > max_length_target:\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>　ありがとう　<end>\n",
      "Input: <start>　ありがとう　<end>\n",
      "Predicted translation: thanks for good . thanks for good . thanks for good . thanks for good \n"
     ]
    }
   ],
   "source": [
    "result, sentence = evaluate(\"ありがとう\") # ありがとう = \"thank you\"\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>　ああ　<end>\n",
      "Input: <start>　ああ　<end>\n",
      "Predicted translation: ah , then . <end> \n"
     ]
    }
   ],
   "source": [
    "result, sentence = evaluate(\"ああ\") # ああ = \"ah\"\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>　これ　は　何　？　<end>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'これ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-61f5d8882a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"これは何？\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# これは何？ = \"what is this?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-279-0b88bd5926ea>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<start>\\u3000\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\u3000<end>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_lang_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\u3000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
      "\u001b[0;32m<ipython-input-279-0b88bd5926ea>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<start>\\u3000\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\u3000<end>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_lang_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\u3000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
      "\u001b[0;31mKeyError\u001b[0m: 'これ'"
     ]
    }
   ],
   "source": [
    "result, sentence = evaluate(\"これは何？\") # これは何？ = \"what is this?\"\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
